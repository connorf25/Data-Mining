{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kcl0YURr1yHO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format xml data\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "\n",
    "abstracts = []\n",
    "tags = []\n",
    "\n",
    "xmldoc = minidom.parse('TrainInclude.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('abstract')\n",
    "for node in xmldoc.getElementsByTagName('abstract'):\n",
    "    abstract = node.getElementsByTagName('style')[0].firstChild.nodeValue\n",
    "    abstracts.append(abstract)\n",
    "    tags.append(1)\n",
    "    \n",
    "xmldoc = minidom.parse('TrainExclude.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('abstract')\n",
    "for node in xmldoc.getElementsByTagName('abstract'):\n",
    "    abstract = node.getElementsByTagName('style')[0].firstChild.nodeValue\n",
    "    abstracts.append(abstract)\n",
    "    tags.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: Urinary tract infections still repr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: Urinary tract infections still repr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This study aims to evaluate the effects of a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This study aims to evaluate the effects of a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: It has been suggested that probiot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                           abstract\n",
       "0     1  OBJECTIVE: Urinary tract infections still repr...\n",
       "1     1  OBJECTIVE: Urinary tract infections still repr...\n",
       "2     1  This study aims to evaluate the effects of a h...\n",
       "3     1  This study aims to evaluate the effects of a h...\n",
       "4     1  BACKGROUND: It has been suggested that probiot..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(tags, abstracts)), \n",
    "               columns =['code', 'abstract'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max length of string:\", df.abstract.map(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scibert_tokenizer is type: <class 'transformers.tokenization_bert.BertTokenizer'>\n",
      "    scibert_model is type: <class 'transformers.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "scibert_model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\",\n",
    "                                  output_hidden_states=True)\n",
    "scibert_tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "print('scibert_tokenizer is type:', type(scibert_tokenizer))\n",
    "print('    scibert_model is type:', type(scibert_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, tokenizer, text):\n",
    "\n",
    "    # Encode with special tokens ([CLS] and [SEP], returning pytorch tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        add_special_tokens = True,\n",
    "                        return_tensors = 'pt'\n",
    "                )\n",
    "\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run through BERT\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # Extract hidden states\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Select the embeddings\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate average of token vectors\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    # Convert to np array\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106 1106\n",
      "8.7\n",
      "1106\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "\n",
    "embeddings = []\n",
    "length = len(df['abstract'].tolist())\n",
    "index = 0\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for sentence in df['abstract'].tolist():\n",
    "    clear_output(wait=True)\n",
    "    index += 1\n",
    "    sen_emb = get_embedding(scibert_model, scibert_tokenizer, sentence)\n",
    "    embeddings.append(sen_emb)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    if (index/length*100) < 1:\n",
    "        expected_time = \"Calculating...\"\n",
    "\n",
    "    else:\n",
    "        time_perc = timeit.default_timer()\n",
    "        expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "    print(index, length)\n",
    "    print(expected_time)\n",
    "\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>abstract</th>\n",
       "      <th>scibert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: Urinary tract infections still repr...</td>\n",
       "      <td>[-0.07536177, -0.06736588, -0.37270182, 0.5410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: Urinary tract infections still repr...</td>\n",
       "      <td>[-0.07536177, -0.06736588, -0.37270182, 0.5410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This study aims to evaluate the effects of a h...</td>\n",
       "      <td>[-0.049732774, 0.14015731, -0.08440088, 0.2876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This study aims to evaluate the effects of a h...</td>\n",
       "      <td>[-0.049732774, 0.14015731, -0.08440088, 0.2876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: It has been suggested that probiot...</td>\n",
       "      <td>[0.22030136, -0.32066062, -0.17968939, 0.64076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: It has been suggested that probiot...</td>\n",
       "      <td>[0.22030136, -0.32066062, -0.17968939, 0.64076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To assess whether the use of simple...</td>\n",
       "      <td>[0.20928083, -0.11177282, -0.23225263, 0.34750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To assess whether the use of simple...</td>\n",
       "      <td>[0.20928083, -0.11177282, -0.23225263, 0.34750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: In the UK, urinary tract infection...</td>\n",
       "      <td>[0.27049634, -0.020403527, -0.40627086, 0.4361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: In the UK, urinary tract infection...</td>\n",
       "      <td>[0.27049634, -0.020403527, -0.40627086, 0.4361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Urinary tract infection (UTI) is a major probl...</td>\n",
       "      <td>[0.09631325, -0.27510288, -0.40678206, 0.54402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Urinary tract infection (UTI) is a major probl...</td>\n",
       "      <td>[0.09631325, -0.27510288, -0.40678206, 0.54402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Importance: Increased hydration is often recom...</td>\n",
       "      <td>[0.21711299, -0.34084395, -0.49678397, 0.52794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Importance: Increased hydration is often recom...</td>\n",
       "      <td>[0.21711299, -0.34084395, -0.49678397, 0.52794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction and Aims:Water intake is recommen...</td>\n",
       "      <td>[-0.024865553, -0.16186136, -0.14873107, 0.579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction and Aims:Water intake is recommen...</td>\n",
       "      <td>[-0.024865553, -0.16186136, -0.14873107, 0.579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>PURPOSE: To test whether D-mannose powder is e...</td>\n",
       "      <td>[0.046218235, -0.06302773, -0.22126898, 0.6498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>PURPOSE: To test whether D-mannose powder is e...</td>\n",
       "      <td>[0.046218235, -0.06302773, -0.22126898, 0.6498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: Urinary tract infections [UTIs] ar...</td>\n",
       "      <td>[0.24679653, -0.3054532, -0.19094783, 0.662071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: Urinary tract infections [UTIs] ar...</td>\n",
       "      <td>[0.24679653, -0.3054532, -0.19094783, 0.662071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To assess the impact of different m...</td>\n",
       "      <td>[0.2736371, -0.2082504, -0.51876724, 0.6482353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To assess the impact of different m...</td>\n",
       "      <td>[0.2736371, -0.2082504, -0.51876724, 0.6482353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>This study was designed to prospectively study...</td>\n",
       "      <td>[0.014235922, -0.05007463, -0.26199156, 0.0388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>This study was designed to prospectively study...</td>\n",
       "      <td>[0.014235922, -0.05007463, -0.26199156, 0.0388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>A total of 112 patients with recurrent lower u...</td>\n",
       "      <td>[0.056562822, -0.2203197, -0.23687795, 0.43115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>A total of 112 patients with recurrent lower u...</td>\n",
       "      <td>[0.056562822, -0.2203197, -0.23687795, 0.43115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction: Recurrent urinary tract infectio...</td>\n",
       "      <td>[0.16017844, 0.11943655, -0.4599876, 0.6457244...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction: Recurrent urinary tract infectio...</td>\n",
       "      <td>[0.16017844, 0.11943655, -0.4599876, 0.6457244...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To compare the efficacy and safety ...</td>\n",
       "      <td>[0.02048144, -0.244254, -0.48355234, 0.4975074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To compare the efficacy and safety ...</td>\n",
       "      <td>[0.02048144, -0.244254, -0.48355234, 0.4975074...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                                           abstract  \\\n",
       "0      1  OBJECTIVE: Urinary tract infections still repr...   \n",
       "1      1  OBJECTIVE: Urinary tract infections still repr...   \n",
       "2      1  This study aims to evaluate the effects of a h...   \n",
       "3      1  This study aims to evaluate the effects of a h...   \n",
       "4      1  BACKGROUND: It has been suggested that probiot...   \n",
       "5      1  BACKGROUND: It has been suggested that probiot...   \n",
       "6      1  OBJECTIVE: To assess whether the use of simple...   \n",
       "7      1  OBJECTIVE: To assess whether the use of simple...   \n",
       "8      1  BACKGROUND: In the UK, urinary tract infection...   \n",
       "9      1  BACKGROUND: In the UK, urinary tract infection...   \n",
       "10     1  Urinary tract infection (UTI) is a major probl...   \n",
       "11     1  Urinary tract infection (UTI) is a major probl...   \n",
       "12     1  Importance: Increased hydration is often recom...   \n",
       "13     1  Importance: Increased hydration is often recom...   \n",
       "14     1  Introduction and Aims:Water intake is recommen...   \n",
       "15     1  Introduction and Aims:Water intake is recommen...   \n",
       "16     1  PURPOSE: To test whether D-mannose powder is e...   \n",
       "17     1  PURPOSE: To test whether D-mannose powder is e...   \n",
       "18     1  BACKGROUND: Urinary tract infections [UTIs] ar...   \n",
       "19     1  BACKGROUND: Urinary tract infections [UTIs] ar...   \n",
       "20     1  OBJECTIVE: To assess the impact of different m...   \n",
       "21     1  OBJECTIVE: To assess the impact of different m...   \n",
       "22     1  This study was designed to prospectively study...   \n",
       "23     1  This study was designed to prospectively study...   \n",
       "24     1  A total of 112 patients with recurrent lower u...   \n",
       "25     1  A total of 112 patients with recurrent lower u...   \n",
       "26     1  Introduction: Recurrent urinary tract infectio...   \n",
       "27     1  Introduction: Recurrent urinary tract infectio...   \n",
       "28     1  OBJECTIVE: To compare the efficacy and safety ...   \n",
       "29     1  OBJECTIVE: To compare the efficacy and safety ...   \n",
       "\n",
       "                                              scibert  \n",
       "0   [-0.07536177, -0.06736588, -0.37270182, 0.5410...  \n",
       "1   [-0.07536177, -0.06736588, -0.37270182, 0.5410...  \n",
       "2   [-0.049732774, 0.14015731, -0.08440088, 0.2876...  \n",
       "3   [-0.049732774, 0.14015731, -0.08440088, 0.2876...  \n",
       "4   [0.22030136, -0.32066062, -0.17968939, 0.64076...  \n",
       "5   [0.22030136, -0.32066062, -0.17968939, 0.64076...  \n",
       "6   [0.20928083, -0.11177282, -0.23225263, 0.34750...  \n",
       "7   [0.20928083, -0.11177282, -0.23225263, 0.34750...  \n",
       "8   [0.27049634, -0.020403527, -0.40627086, 0.4361...  \n",
       "9   [0.27049634, -0.020403527, -0.40627086, 0.4361...  \n",
       "10  [0.09631325, -0.27510288, -0.40678206, 0.54402...  \n",
       "11  [0.09631325, -0.27510288, -0.40678206, 0.54402...  \n",
       "12  [0.21711299, -0.34084395, -0.49678397, 0.52794...  \n",
       "13  [0.21711299, -0.34084395, -0.49678397, 0.52794...  \n",
       "14  [-0.024865553, -0.16186136, -0.14873107, 0.579...  \n",
       "15  [-0.024865553, -0.16186136, -0.14873107, 0.579...  \n",
       "16  [0.046218235, -0.06302773, -0.22126898, 0.6498...  \n",
       "17  [0.046218235, -0.06302773, -0.22126898, 0.6498...  \n",
       "18  [0.24679653, -0.3054532, -0.19094783, 0.662071...  \n",
       "19  [0.24679653, -0.3054532, -0.19094783, 0.662071...  \n",
       "20  [0.2736371, -0.2082504, -0.51876724, 0.6482353...  \n",
       "21  [0.2736371, -0.2082504, -0.51876724, 0.6482353...  \n",
       "22  [0.014235922, -0.05007463, -0.26199156, 0.0388...  \n",
       "23  [0.014235922, -0.05007463, -0.26199156, 0.0388...  \n",
       "24  [0.056562822, -0.2203197, -0.23687795, 0.43115...  \n",
       "25  [0.056562822, -0.2203197, -0.23687795, 0.43115...  \n",
       "26  [0.16017844, 0.11943655, -0.4599876, 0.6457244...  \n",
       "27  [0.16017844, 0.11943655, -0.4599876, 0.6457244...  \n",
       "28  [0.02048144, -0.244254, -0.48355234, 0.4975074...  \n",
       "29  [0.02048144, -0.244254, -0.48355234, 0.4975074...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['scibert'] = embeddings\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to prevent recalculation\n",
    "df.to_pickle(\"./df_screeningUTI.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>abstract</th>\n",
       "      <th>scibert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: Urinary tract infections still repr...</td>\n",
       "      <td>[-0.07536177, -0.06736588, -0.37270182, 0.5410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: Urinary tract infections still repr...</td>\n",
       "      <td>[-0.07536177, -0.06736588, -0.37270182, 0.5410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This study aims to evaluate the effects of a h...</td>\n",
       "      <td>[-0.049732774, 0.14015731, -0.08440088, 0.2876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This study aims to evaluate the effects of a h...</td>\n",
       "      <td>[-0.049732774, 0.14015731, -0.08440088, 0.2876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: It has been suggested that probiot...</td>\n",
       "      <td>[0.22030136, -0.32066062, -0.17968939, 0.64076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: It has been suggested that probiot...</td>\n",
       "      <td>[0.22030136, -0.32066062, -0.17968939, 0.64076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To assess whether the use of simple...</td>\n",
       "      <td>[0.20928083, -0.11177282, -0.23225263, 0.34750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>OBJECTIVE: To assess whether the use of simple...</td>\n",
       "      <td>[0.20928083, -0.11177282, -0.23225263, 0.34750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: In the UK, urinary tract infection...</td>\n",
       "      <td>[0.27049634, -0.020403527, -0.40627086, 0.4361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: In the UK, urinary tract infection...</td>\n",
       "      <td>[0.27049634, -0.020403527, -0.40627086, 0.4361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Urinary tract infection (UTI) is a major probl...</td>\n",
       "      <td>[0.09631325, -0.27510288, -0.40678206, 0.54402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Urinary tract infection (UTI) is a major probl...</td>\n",
       "      <td>[0.09631325, -0.27510288, -0.40678206, 0.54402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Importance: Increased hydration is often recom...</td>\n",
       "      <td>[0.21711299, -0.34084395, -0.49678397, 0.52794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Importance: Increased hydration is often recom...</td>\n",
       "      <td>[0.21711299, -0.34084395, -0.49678397, 0.52794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction and Aims:Water intake is recommen...</td>\n",
       "      <td>[-0.024865553, -0.16186136, -0.14873107, 0.579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction and Aims:Water intake is recommen...</td>\n",
       "      <td>[-0.024865553, -0.16186136, -0.14873107, 0.579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>PURPOSE: To test whether D-mannose powder is e...</td>\n",
       "      <td>[0.046218235, -0.06302773, -0.22126898, 0.6498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>PURPOSE: To test whether D-mannose powder is e...</td>\n",
       "      <td>[0.046218235, -0.06302773, -0.22126898, 0.6498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: Urinary tract infections [UTIs] ar...</td>\n",
       "      <td>[0.24679653, -0.3054532, -0.19094783, 0.662071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>BACKGROUND: Urinary tract infections [UTIs] ar...</td>\n",
       "      <td>[0.24679653, -0.3054532, -0.19094783, 0.662071...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                                           abstract  \\\n",
       "0      1  OBJECTIVE: Urinary tract infections still repr...   \n",
       "1      1  OBJECTIVE: Urinary tract infections still repr...   \n",
       "2      1  This study aims to evaluate the effects of a h...   \n",
       "3      1  This study aims to evaluate the effects of a h...   \n",
       "4      1  BACKGROUND: It has been suggested that probiot...   \n",
       "5      1  BACKGROUND: It has been suggested that probiot...   \n",
       "6      1  OBJECTIVE: To assess whether the use of simple...   \n",
       "7      1  OBJECTIVE: To assess whether the use of simple...   \n",
       "8      1  BACKGROUND: In the UK, urinary tract infection...   \n",
       "9      1  BACKGROUND: In the UK, urinary tract infection...   \n",
       "10     1  Urinary tract infection (UTI) is a major probl...   \n",
       "11     1  Urinary tract infection (UTI) is a major probl...   \n",
       "12     1  Importance: Increased hydration is often recom...   \n",
       "13     1  Importance: Increased hydration is often recom...   \n",
       "14     1  Introduction and Aims:Water intake is recommen...   \n",
       "15     1  Introduction and Aims:Water intake is recommen...   \n",
       "16     1  PURPOSE: To test whether D-mannose powder is e...   \n",
       "17     1  PURPOSE: To test whether D-mannose powder is e...   \n",
       "18     1  BACKGROUND: Urinary tract infections [UTIs] ar...   \n",
       "19     1  BACKGROUND: Urinary tract infections [UTIs] ar...   \n",
       "\n",
       "                                              scibert  \n",
       "0   [-0.07536177, -0.06736588, -0.37270182, 0.5410...  \n",
       "1   [-0.07536177, -0.06736588, -0.37270182, 0.5410...  \n",
       "2   [-0.049732774, 0.14015731, -0.08440088, 0.2876...  \n",
       "3   [-0.049732774, 0.14015731, -0.08440088, 0.2876...  \n",
       "4   [0.22030136, -0.32066062, -0.17968939, 0.64076...  \n",
       "5   [0.22030136, -0.32066062, -0.17968939, 0.64076...  \n",
       "6   [0.20928083, -0.11177282, -0.23225263, 0.34750...  \n",
       "7   [0.20928083, -0.11177282, -0.23225263, 0.34750...  \n",
       "8   [0.27049634, -0.020403527, -0.40627086, 0.4361...  \n",
       "9   [0.27049634, -0.020403527, -0.40627086, 0.4361...  \n",
       "10  [0.09631325, -0.27510288, -0.40678206, 0.54402...  \n",
       "11  [0.09631325, -0.27510288, -0.40678206, 0.54402...  \n",
       "12  [0.21711299, -0.34084395, -0.49678397, 0.52794...  \n",
       "13  [0.21711299, -0.34084395, -0.49678397, 0.52794...  \n",
       "14  [-0.024865553, -0.16186136, -0.14873107, 0.579...  \n",
       "15  [-0.024865553, -0.16186136, -0.14873107, 0.579...  \n",
       "16  [0.046218235, -0.06302773, -0.22126898, 0.6498...  \n",
       "17  [0.046218235, -0.06302773, -0.22126898, 0.6498...  \n",
       "18  [0.24679653, -0.3054532, -0.19094783, 0.662071...  \n",
       "19  [0.24679653, -0.3054532, -0.19094783, 0.662071...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./df_screeningUTI.pkl\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample even number from each class\n",
    "even_df = df.groupby('code').apply(lambda x: x.sample(n=25)).reset_index(drop=True)\n",
    "even_df = even_df.sample(frac=1)\n",
    "even_df.head(50)\n",
    "train_data = even_df['scibert'].tolist()\n",
    "train_labels = even_df['code'].tolist()\n",
    "\n",
    "# Sample even number from each class\n",
    "test_df = df.groupby('code').apply(lambda x: x.sample(n=25)).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1)\n",
    "test_df.head(50)\n",
    "test_data = test_df['scibert'].tolist()\n",
    "test_labels = test_df['code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# Fit models\n",
    "# Predictions for each model\n",
    "\n",
    "f1 = []\n",
    "mcc = []\n",
    "acc = []\n",
    "\n",
    "for i in range(51):\n",
    "    if i > 2:\n",
    "        model.fit(train_data[:i], train_labels[:i])\n",
    "        pred = model.predict(test_data)\n",
    "        # Evaluate\n",
    "        f1.append(f1_score(test_labels, pred, pos_label=1))\n",
    "        mcc.append(matthews_corrcoef(test_labels, pred))\n",
    "        acc.append(accuracy_score(test_labels, pred))\n",
    "\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(acc[-1])\n",
    "        \n",
    "plt.figure(figsize = (15,8))\n",
    "plt.xticks(np.arange(0, 50, step=5))\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "f, = plt.plot(range(50)[2:], f1)\n",
    "# m, = plt.plot(range(50)[2:], mcc)\n",
    "a, = plt.plot(range(50)[2:], acc)\n",
    "plt.legend([f, a], [\"f1\", \"accuracy\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample even number from each class\n",
    "even_df = df.groupby('code').apply(lambda x: x.sample(n=25)).reset_index(drop=True)\n",
    "even_df = even_df.sample(frac=1)\n",
    "even_df.head(50)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(even_df, test_size=0.2)\n",
    "\n",
    "train_data = train['scibert'].tolist()\n",
    "train_labels = train['code'].tolist()\n",
    "\n",
    "test_data = test['scibert'].tolist()\n",
    "test_labels = test['code'].tolist()\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# Fit models\n",
    "# Predictions for each model\n",
    "\n",
    "f1 = []\n",
    "mcc = []\n",
    "acc = []\n",
    "\n",
    "for i in range(41):\n",
    "    if i > 2:\n",
    "        model.fit(train_data[:i], train_labels[:i])\n",
    "        pred = model.predict(test_data)\n",
    "        # Evaluate\n",
    "        f1.append(f1_score(test_labels, pred, pos_label=1))\n",
    "        mcc.append(matthews_corrcoef(test_labels, pred))\n",
    "        acc.append(accuracy_score(test_labels, pred))\n",
    "#     print(confusion_matrix(test_labels, pred))\n",
    "       \n",
    "conf_matrix = confusion_matrix(test_labels, pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.yticks(np.arange(0, 1, step=0.1))\n",
    "f, = plt.plot(range(40)[2:], f1)\n",
    "m, = plt.plot(range(40)[2:], mcc)\n",
    "a, = plt.plot(range(40)[2:], acc)\n",
    "plt.legend([f, m, a], [\"f1\", \"mcc\", \"accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>abstract</th>\n",
       "      <th>scibert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "      <td>BACKGROUND: Puerperal sepsis is frequently in ...</td>\n",
       "      <td>[0.18116444, -0.11233642, -0.033121966, 0.6906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0</td>\n",
       "      <td>One hundred twenty-nine patients undergoing tr...</td>\n",
       "      <td>[0.30153793, -0.3384084, -0.17920418, 0.392208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0</td>\n",
       "      <td>Forty-two cases of children showing recurrent ...</td>\n",
       "      <td>[-0.05956588, -0.13077176, -0.10262503, 0.4628...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0</td>\n",
       "      <td>To study the effects of continous low doses of...</td>\n",
       "      <td>[0.09397999, -0.42214558, -0.18888585, 0.45590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0</td>\n",
       "      <td>Suppressive therapy with antibiotics has long ...</td>\n",
       "      <td>[0.26457232, -0.3831795, -0.14558473, 0.628578...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     code                                           abstract  \\\n",
       "895     0  BACKGROUND: Puerperal sepsis is frequently in ...   \n",
       "301     0  One hundred twenty-nine patients undergoing tr...   \n",
       "836     0  Forty-two cases of children showing recurrent ...   \n",
       "463     0  To study the effects of continous low doses of...   \n",
       "840     0  Suppressive therapy with antibiotics has long ...   \n",
       "\n",
       "                                               scibert  \n",
       "895  [0.18116444, -0.11233642, -0.033121966, 0.6906...  \n",
       "301  [0.30153793, -0.3384084, -0.17920418, 0.392208...  \n",
       "836  [-0.05956588, -0.13077176, -0.10262503, 0.4628...  \n",
       "463  [0.09397999, -0.42214558, -0.18888585, 0.45590...  \n",
       "840  [0.26457232, -0.3831795, -0.14558473, 0.628578...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = df.sample(frac=1)\n",
    "train_data = all_df['scibert'].tolist()\n",
    "train_labels = all_df['code'].tolist()\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=2.5, max_iter=1000)\n",
    "\n",
    "model.fit(train_data, train_labels)\n",
    "pred = model.predict(train_data)\n",
    "       \n",
    "conf_matrix = confusion_matrix(train_labels, pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "labels = [\"Exclude\", \"Include\"]\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,2))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1072    0]\n",
      " [   0   34]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "model.fit(train_data, train_labels)\n",
    "pred = model.predict(train_data)\n",
    "       \n",
    "conf_matrix = confusion_matrix(train_labels, pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "labels = [\"Exclude\", \"Include\"]\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,2))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "0.04\n",
      "5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c940e67192d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "xmldoc = minidom.parse('TestInclude.xml')\n",
    "abstracts = []\n",
    "tags = []\n",
    "\n",
    "itemlist = xmldoc.getElementsByTagName('abstract')\n",
    "for node in xmldoc.getElementsByTagName('abstract'):\n",
    "    abstract = node.getElementsByTagName('style')[0].firstChild.nodeValue\n",
    "    abstracts.append(abstract)\n",
    "    tags.append(1)\n",
    "    \n",
    "test_df = pd.DataFrame(list(zip(tags, abstracts)), \n",
    "               columns =['code', 'abstract'])\n",
    "test_df.head()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "\n",
    "embeddings = []\n",
    "length = len(test_df['abstract'].tolist())\n",
    "index = 0\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for sentence in test_df['abstract'].tolist():\n",
    "    clear_output(wait=True)\n",
    "    index += 1\n",
    "    sen_emb = get_embedding(scibert_model, scibert_tokenizer, sentence)\n",
    "    embeddings.append(sen_emb)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    if (index/length*100) < 1:\n",
    "        expected_time = \"Calculating...\"\n",
    "\n",
    "    else:\n",
    "        time_perc = timeit.default_timer()\n",
    "        expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "    print(index, length)\n",
    "    print(expected_time)\n",
    "\n",
    "print(len(embeddings))\n",
    "\n",
    "test_df['scibert'] = embeddings\n",
    "test_df.head(30)\n",
    "\n",
    "test_df = test_df.sample(frac=1)\n",
    "test_data = test_df['scibert'].tolist()\n",
    "test_labels = test_df['code'].tolist()\n",
    "test_df.head()\n",
    "\n",
    "pred = model.predict(test_data)\n",
    "       \n",
    "conf_matrix = confusion_matrix(test_labels, pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "labels = [\"Exclude\", \"Include\"]\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,2))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg9vHsH0pn_U"
   },
   "source": [
    "# Import and Preprocess/Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aFAYwJFHPwp"
   },
   "outputs": [],
   "source": [
    "# Function which does everything below at once\n",
    "def open_format_data(filename):\n",
    "  # Save data from txt into list\n",
    "  f = open(filename, \"r\")\n",
    "\n",
    "  list_data = []\n",
    "  for x in f:\n",
    "      list_data.append(f.readline().rstrip().split(\"\\t\"))\n",
    "\n",
    "  # Load list into dataframe\n",
    "  column_names = ['label', \"sentence\"]\n",
    "  df = pd.DataFrame(list_data, columns= column_names)\n",
    "  df.head(10)\n",
    "\n",
    "  # Drop any null values\n",
    "  df = df.dropna()\n",
    "\n",
    "  # Drop duplicates\n",
    "  df = df.drop_duplicates()\n",
    "\n",
    "  # Append number representing label as code\n",
    "  df.label = pd.Categorical(df.label)\n",
    "  df['code'] = df.label.cat.codes\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyP98KpnHvv0"
   },
   "outputs": [],
   "source": [
    "df = open_format_data(\"train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0QiFmEMphvr"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "FiwPvz1bpy91",
    "outputId": "22af54a2-4c23-4801-9700-cbe23b8f9a37"
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "nIadEZxtqNDg",
    "outputId": "9ef35a7c-7ea5-428d-a80c-98059140eebc"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "2EIxdIgOkNiW",
    "outputId": "73d79ed3-a443-43e6-d368-f0b0a139345e"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "wBk2y4aeVnl_",
    "outputId": "8f55898b-2d00-48c9-ab3e-5e29e7c3e75d"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"label\")['sentence'].count().plot.bar()\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF4bB9HVCGGR"
   },
   "source": [
    "# SciBERT Model For Sentence Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "tEWpbCTNCJbp",
    "outputId": "49109daa-5430-4f7d-f07e-ee3b5bfdb486"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "QyIfIV1jCV9B",
    "outputId": "43b93ee8-ff1e-4f30-da57-cb1a894721cc"
   },
   "outputs": [],
   "source": [
    "scibert_model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\",\n",
    "                                  output_hidden_states=True)\n",
    "scibert_tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "print('scibert_tokenizer is type:', type(scibert_tokenizer))\n",
    "print('    scibert_model is type:', type(scibert_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug_GZMKjFnIK"
   },
   "outputs": [],
   "source": [
    "def get_embedding(model, tokenizer, text):\n",
    "\n",
    "    # Encode with special tokens ([CLS] and [SEP], returning pytorch tensors\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens = True,\n",
    "                        return_tensors = 'pt'\n",
    "                )\n",
    "\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run through BERT\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # Extract hidden states\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Select the embeddings\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate average of token vectors\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    # Convert to np array\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "4WL7iwzGKVHX",
    "outputId": "a9c2aadb-f3c8-4343-ee30-fbf9a19d0549"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "\n",
    "embeddings = []\n",
    "length = len(df['sentence'].tolist())\n",
    "index = 0\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for sentence in df['sentence'].tolist():\n",
    "    clear_output(wait=True)\n",
    "    index += 1\n",
    "    sen_emb = get_embedding(scibert_model, scibert_tokenizer, sentence)\n",
    "    embeddings.append(sen_emb)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    if (index/length*100) < 1:\n",
    "        expected_time = \"Calculating...\"\n",
    "\n",
    "    else:\n",
    "        time_perc = timeit.default_timer()\n",
    "        expected_time = np.round( (time_perc-start) /(index/length) /60,2)\n",
    "\n",
    "    print(index, length)\n",
    "    print(expected_time)\n",
    "\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmZkrrRX_CGP"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXAjeF4U-ydM"
   },
   "outputs": [],
   "source": [
    "df['scibert'] = embeddings\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7kcDqCn_GnI"
   },
   "outputs": [],
   "source": [
    "# Save dataframe to prevent recalculation\n",
    "df.to_pickle(\"./df_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating SciBert Embedded Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "df = pd.read_pickle(\"./df_embeddings.pkl\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to weka\n",
    "export_df = pd.DataFrame(df['scibert'][:2500].tolist(), index=df.index[:2500])\n",
    "export_df['label'] = df['label'][:2500]\n",
    "\n",
    "export_df.to_csv('./weka.csv', index=False)\n",
    "\n",
    "export_df = pd.DataFrame(df['scibert'][2500:3500].tolist(), index=df.index[2500:3500])\n",
    "export_df['label'] = df['label'][2500:3500]\n",
    "export_df.to_csv('./weka_test.csv', index=False)\n",
    "\n",
    "# Weka Metrics\n",
    "labels = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
    "conf_matrix = [[72, 12, 13, 28, 2], \n",
    "               [18, 99, 7, 7, 28], \n",
    "               [6, 0, 306, 10, 18],\n",
    "               [16, 7, 10, 33, 0],\n",
    "               [ 1, 4, 16, 0, 287]]\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,2))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN with scikit-learn\n",
    "import timeit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(8, metric=\"manhattan\", n_jobs=-1)\n",
    "# Fit models\n",
    "start = timeit.default_timer()\n",
    "model.fit(df['scibert'][:2500].tolist(), df['label'][:2500].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time elapsed:\", stop-start)\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['label'][2500:3500].tolist(), pred, average=\"weighted\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['label'][2500:3500].tolist(), pred))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['label'][2500:3500].tolist(), pred))\n",
    "print(classification_report(df['label'][2500:3500].tolist(), pred))\n",
    "# Confusion Matrix\n",
    "labels = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
    "conf_matrix = confusion_matrix(df['label'][2500:3500].tolist(), pred, labels=labels)\n",
    "# perc_matrix = conf_matrix/np.sum(conf_matrix)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (5,3))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN from scratch\n",
    "import timeit\n",
    "import heapq\n",
    "\n",
    "def knn(k, test_data, train_data, train_label):\n",
    "    pred = []\n",
    "    # Iterate over each input data case\n",
    "    for test_embedding in test_data[:20]:\n",
    "        # List to hold (distance, label)\n",
    "        distances = []\n",
    "        # Calculate distance of data point from each train_data case\n",
    "        for i in range(len(train_data)):\n",
    "            manhattan_dist = 0\n",
    "            # Iterate over each dimension of the embedding\n",
    "            for x in range(len(test_embedding)):\n",
    "                manhattan_dist += abs(test_embedding[x] - train_data[i][x])\n",
    "            # Append (distance, label)\n",
    "            distances.append((manhattan_dist, train_label[i]))\n",
    "        # Find k-nearest neighbors\n",
    "        k_smallest = heapq.nsmallest(k, distances)\n",
    "        # Select just labels\n",
    "        labels = [lis[1] for lis in k_smallest]\n",
    "        # Find mode (in tie will select first appearing neighbor)\n",
    "        label = max(set(labels), key=labels.count)\n",
    "        pred.append(label)\n",
    "    return pred\n",
    "            \n",
    "train_data = df['scibert'][:2500].tolist()\n",
    "train_label = df['code'][:2500].tolist()\n",
    "test_data = df['scibert'][2500:3500].tolist()\n",
    "test_labels = df['code'][2500:3500].tolist()\n",
    "\n",
    "# Run knn\n",
    "start = timeit.default_timer()\n",
    "pred = knn(5, test_data, train_data, train_label)\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time elapsed:\", stop-start)\n",
    "\n",
    "# Evaluate\n",
    "# print(\"F1 Score\")\n",
    "# print(f1_score(test_labels[:20], pred, average=\"macro\"))\n",
    "# print(\"MCC\")\n",
    "# print(matthews_corrcoef(test_labels[:20], pred))\n",
    "# print(\"Accuracy\")\n",
    "# print(accuracy_score(test_labels[:20], pred))\n",
    "# print(confusion_matrix(test_labels[:20], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN improved with numpy\n",
    "import timeit\n",
    "import heapq\n",
    "# import Numpy for more efficient manhattan distance\n",
    "import numpy as np\n",
    "\n",
    "def knn(k, test_data, train_data, train_label):\n",
    "    test_arr = np.array(test_data)\n",
    "    train_arr = np.array(train_data)\n",
    "    pred = []\n",
    "    # Iterate over each input data case\n",
    "    for test_embedding in test_arr:\n",
    "        # List to hold (distance, label)\n",
    "        distances = []\n",
    "        # Calculate distance of data point from each train_data case\n",
    "        for i in range(len(train_arr)):\n",
    "            manhattan_dist = np.sum(np.abs(test_embedding-train_arr[i]))\n",
    "            # Append (distance, label)\n",
    "            distances.append((manhattan_dist, train_label[i]))\n",
    "        # Find k-nearest neighbors\n",
    "        k_smallest = heapq.nsmallest(k, distances)\n",
    "        # Select just labels\n",
    "        labels = [lis[1] for lis in k_smallest]\n",
    "        # Find mode (in tie will select first appearing neighbor)\n",
    "        label = max(set(labels), key=labels.count)\n",
    "        pred.append(label)\n",
    "    return pred\n",
    "            \n",
    "train_data = df['scibert'][:2500].tolist()\n",
    "train_label = df['code'][:2500].tolist()\n",
    "test_data = df['scibert'][2500:3500].tolist()\n",
    "\n",
    "# Run knn\n",
    "start = timeit.default_timer()\n",
    "pred = knn(8, test_data, train_data, train_label)\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time elapsed:\", stop-start)\n",
    "\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"weighted\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(classification_report(df['code'][2500:3500].tolist(), pred))\n",
    "# Confusion Matrix\n",
    "labels = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
    "conf_matrix = confusion_matrix(df['code'][2500:3500].tolist(), pred)\n",
    "# perc_matrix = conf_matrix/np.sum(conf_matrix)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,3))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample even number from each class\n",
    "even_df = df.groupby('code').apply(lambda x: x.sample(n=500))\n",
    "even_df = even_df[(even_df['label'] == 'METHODS') | (even_df['label'] == 'OBJECTIVE')].reset_index(drop = True)\n",
    "even_df.label.cat.remove_unused_categories(True)\n",
    "even_df = even_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "even_test = df.groupby('code').apply(lambda x: x.sample(n=200))\n",
    "even_test = even_test[(even_test['label'] == 'METHODS') | (even_test['label'] == 'OBJECTIVE')].reset_index(drop = True)\n",
    "even_test.label.cat.remove_unused_categories(True)\n",
    "even_test = even_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# KNN with balanced data\n",
    "train_data = even_df['scibert'].tolist()\n",
    "train_labels = even_df['label'].tolist()\n",
    "test_data = even_test['scibert'].tolist()\n",
    "test_labels = even_test['label'].tolist()\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_even = KNeighborsClassifier(metric=\"manhattan\", n_jobs=-1)\n",
    "model_even.fit(train_data, train_labels)\n",
    "pred_even = model_even.predict(test_data)\n",
    "\n",
    "print(f1_score(test_labels, pred_even, average=\"macro\"))\n",
    "print(matthews_corrcoef(test_labels, pred_even))\n",
    "print(accuracy_score(test_labels, pred_even))\n",
    "print(classification_report(test_labels, pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB()\n",
    "model_even = BernoulliNB()\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model_even = GaussianNB()\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model_even = DecisionTreeClassifier()\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(multi_class=\"ovr\", max_iter=1000)\n",
    "model_even = LinearSVC(multi_class=\"ovr\", max_iter=1000)\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf', gamma=0.02, C=2)\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['label'][:2500].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['label'][2500:3500].tolist(), pred, average=\"weighted\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['label'][2500:3500].tolist(), pred))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['label'][2500:3500].tolist(), pred))\n",
    "print(classification_report(df['label'][2500:3500].tolist(), pred))\n",
    "# Confusion Matrix\n",
    "labels = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
    "conf_matrix = confusion_matrix(df['label'][2500:3500].tolist(), pred)\n",
    "# perc_matrix = conf_matrix/np.sum(conf_matrix)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,3))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# model = SVC(kernel='rbf', gamma=0.02, C=2)\n",
    "# from sklearn.svm import LinearSVC\n",
    "# model = LinearSVC()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# Fit models\n",
    "# Predictions for each model\n",
    "\n",
    "f1 = []\n",
    "mcc = []\n",
    "acc = []\n",
    "\n",
    "for i in range(51):\n",
    "    if i > 2:\n",
    "        model.fit(train_data[:i], train_labels[:i])\n",
    "        pred = model.predict(test_data)\n",
    "        # Evaluate\n",
    "        f1.append(f1_score(test_labels, pred, pos_label=\"METHODS\"))\n",
    "        mcc.append(matthews_corrcoef(test_labels, pred))\n",
    "        acc.append(accuracy_score(test_labels, pred))\n",
    "#     print(confusion_matrix(test_labels, pred))\n",
    "        \n",
    "plt.figure(figsize = (15,8))\n",
    "plt.yticks(np.arange(0, 1, step=0.1))\n",
    "f, = plt.plot(range(50)[2:], f1)\n",
    "m, = plt.plot(range(50)[2:], mcc)\n",
    "a, = plt.plot(range(50)[2:], acc)\n",
    "plt.legend([f, m, a], [\"f1\", \"mcc\", \"accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:25000].tolist(), df['label'][:25000].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][25000:35000].tolist())\n",
    "\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['label'][25000:35000].tolist(), pred, average=\"weighted\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['label'][25000:35000].tolist(), pred))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['label'][25000:35000].tolist(), pred))\n",
    "print(classification_report(df['label'][25000:35000].tolist(), pred))\n",
    "# Confusion Matrix\n",
    "labels = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
    "conf_matrix = confusion_matrix(df['label'][25000:35000].tolist(), pred)\n",
    "# perc_matrix = conf_matrix/np.sum(conf_matrix)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,3))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf', gamma=0.02, C=2)\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:25000].tolist(), df['label'][:25000].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][25000:35000].tolist())\n",
    "\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['label'][25000:35000].tolist(), pred, average=\"weighted\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['label'][25000:35000].tolist(), pred))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['label'][25000:35000].tolist(), pred))\n",
    "print(classification_report(df['label'][25000:35000].tolist(), pred))\n",
    "# Confusion Matrix\n",
    "labels = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
    "conf_matrix = confusion_matrix(df['label'][25000:35000].tolist(), pred)\n",
    "# perc_matrix = conf_matrix/np.sum(conf_matrix)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=labels, index=labels)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (4,3))\n",
    "sns.set(font_scale=1.2)#for label size\n",
    "sns.heatmap(df_cm, fmt='d', cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even data\n",
    "from sklearn.svm import SVC\n",
    "model_even = SVC(kernel='rbf', gamma=0.02, C=2)\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "pred_even = model_even.predict(even_test['scibert'].tolist())\n",
    "print(f1_score(even_test['code'].tolist(), pred_even, average=\"macro\"))\n",
    "print(matthews_corrcoef(even_test['code'].tolist(), pred_even))\n",
    "print(accuracy_score(even_test['code'].tolist(), pred_even))\n",
    "print(confusion_matrix(even_test['code'].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class=\"ovr\", max_iter=1000)\n",
    "model_even = LogisticRegression(multi_class=\"ovr\", max_iter=1000)\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(confusion_matrix(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier()\n",
    "model_even = MLPClassifier()\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model = Perceptron()\n",
    "model_even = Perceptron()\n",
    "# Fit models\n",
    "model.fit(df['scibert'][:2500].tolist(), df['code'][:2500].tolist())\n",
    "model_even.fit(even_df['scibert'].tolist(), even_df['code'].tolist())\n",
    "# Predictions for each model\n",
    "pred = model.predict(df['scibert'][2500:3500].tolist())\n",
    "pred_even = model_even.predict(df['scibert'][2500:3500].tolist())\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred_even, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred_even))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred_even))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow NN\n",
    "import numpy as np\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(768, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer. The first argument is the number of labels.\n",
    "model.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_em = df['scibert'][:2500]\n",
    "train_em = np.array(train_em.tolist())\n",
    "train_label = df['code'][:2500]\n",
    "train_label = np.array(train_label.tolist())\n",
    "\n",
    "test_em = df['scibert'][2500:3500]\n",
    "test_em = np.array(test_em.tolist())\n",
    "test_label = df['code'][2500:3500]\n",
    "test_label = np.array(test_label.tolist())\n",
    "\n",
    "# Fit Model\n",
    "model.fit(train_em, train_label, epochs=4, validation_data=(test_em, test_label))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(test_em, test_label)\n",
    "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))\n",
    "\n",
    "pred = model.predict(test_em).argmax(axis=-1)\n",
    "\n",
    "# Evaluate\n",
    "print(\"F1 Score\")\n",
    "print(f1_score(df['code'][2500:3500].tolist(), pred, average=\"macro\"))\n",
    "print(\"MCC\")\n",
    "print(matthews_corrcoef(df['code'][2500:3500].tolist(), pred))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(df['code'][2500:3500].tolist(), pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(df['code'][2500:3500].tolist(), pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pubmed_20k_Pandas.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
